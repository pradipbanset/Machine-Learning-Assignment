{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10771267,"sourceType":"datasetVersion","datasetId":6682176}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Loading the necessary packages!\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport optuna\nimport xgboost as xgb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, classification_report, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, median_absolute_error\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom scipy import stats\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport catboost as cb\nfrom scipy.optimize import minimize","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loasding the data!!\ntrain = pd.read_csv(\"/kaggle/input/bagpack11/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/bagpack11/test.csv\")\nextra_train = pd.read_csv(\"/kaggle/input/bagpack11/training_extra.csv\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extra_train.head(10)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#checking the number of rows and column present in the data\n\nnum_train_rows, num_train_columns = train.shape\n\nnum_test_rows, num_test_columns = test.shape\n\nnum_extra_rows, num_extra_columns = extra_train.shape\n\nprint(\"Training Data:\")\nprint(f\"Number of Rows: {num_train_rows}\")\nprint(f\"Number of Columns: {num_train_columns}\\n\")\n\nprint(\"Test Data:\")\nprint(f\"Number of Rows: {num_test_rows}\")\nprint(f\"Number of Columns: {num_test_columns}\\n\")\n\nprint(\"Extra Data:\")\nprint(f\"Number of Rows: {num_extra_rows}\")\nprint(f\"Number of Columns: {num_extra_columns}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This below code creates a summary table that displays information about missing values, unique values, and data types for features in multiple datasets (train, test, extra_train)","metadata":{"editable":false}},{"cell_type":"code","source":"# Creating a table for missing values, unique values and data types of the features\n\nmissing_values_train = pd.DataFrame({'Feature': train.columns,\n                              '[TRAIN] No. of Missing Values': train.isnull().sum().values,\n                              '[TRAIN] % of Missing Values': ((train.isnull().sum().values)/len(train)*100)})\n\nmissing_values_test = pd.DataFrame({'Feature': test.columns,\n                             '[TEST] No.of Missing Values': test.isnull().sum().values,\n                             '[TEST] % of Missing Values': ((test.isnull().sum().values)/len(test)*100)})\n\nmissing_values_original = pd.DataFrame({'Feature': extra_train.columns,\n                             '[ORIGINAL] No.of Missing Values': extra_train.isnull().sum().values,\n                             '[ORIGINAL] % of Missing Values': ((extra_train.isnull().sum().values)/len(extra_train)*100)})\n\nunique_values = pd.DataFrame({'Feature': train.columns,\n                              'No. of Unique Values[FROM TRAIN]': train.nunique().values})\nfeature_types = pd.DataFrame({'Feature': train.columns,\n                              'DataType': train.dtypes})\n\nmerged_df = pd.merge(missing_values_train, missing_values_test, on='Feature', how='left')\nmerged_df = pd.merge(merged_df, missing_values_original, on='Feature', how='left')\nmerged_df = pd.merge(merged_df, unique_values, on='Feature', how='left')\nmerged_df = pd.merge(merged_df, feature_types, on='Feature', how='left')\n\nmerged_df","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Count the duplicates in the train test and extra train \ntrain_duplicates = train.duplicated().sum()\ntest_duplicates = test.duplicated().sum()\nextra_duplicates = extra_train.duplicated().sum()\n\nprint(f\"Number of duplicate rows in train data: {train_duplicates}\")\nprint(f\"Number of duplicate rows in test data: {test_duplicates}\")\nprint(f\"Number of duplicate rows in extra train data: {extra_duplicates}\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe().T","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EDA","metadata":{"editable":false}},{"cell_type":"code","source":"train = pd.concat([train, extra_train], ignore_index=True)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_variables = [\"Weight Capacity (kg)\"]\ntarget_variable = \"Price\"\ncategorical_variables = ['Brand', 'Material', 'Size', 'Compartments', 'Laptop Compartment','Waterproof', 'Style', 'Color']","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Imputation (Handling missing values)\n\nUnderstanding Each Feature and Missing Data Handling\n1️⃣ Brand (~3.24% missing in train, ~3.11% in test)\nUnique Values: Jansport, Under Armour, Nike, Adidas, Puma, NaN\nImputation Strategy: Since it's categorical with only 5 unique brands, we can impute missing values with the mode (most frequent brand).\n2️⃣ Material (~2.78% missing in train, ~2.81% in test)\nUnique Values: Leather, Canvas, Nylon, Polyester, NaN\nImputation Strategy: Mode imputation works best here since materials are limited categories.\n3️⃣ Size (~2.20% missing in train, ~2.19% in test)\nUnique Values: Medium, Small, Large, NaN\nImputation Strategy: Since it's a well-defined categorical variable, we use mode imputation.\n4️⃣ Compartments (✅ No missing values)\nUnique Values: 10 unique numerical values\nImputation Strategy: ✅ No action needed.\n5️⃣ Laptop Compartment (~2.48% missing in train, ~2.48% in test)\nUnique Values: Yes, No, NaN\nImputation Strategy: Since it's a binary categorical variable, we use mode imputation.\n6️⃣ Waterproof (~2.35% missing in train, ~2.41% in test)\nUnique Values: Yes, No, NaN\nImputation Strategy: Mode imputation is best.\n7️⃣ Style (~2.66% missing in train, ~2.58% in test)\nUnique Values: Tote, Messenger, Backpack, NaN\nImputation Strategy: Mode imputation.\n8️⃣ Color (~3.32% missing in train, ~3.39% in test)\nUnique Values: Black, Green, Red, Blue, Gray, Pink, NaN\nImputation Strategy: Mode imputation.\n9️⃣ Weight Capacity (kg) (~0.05% missing in train, ~0.04% in test)\nUnique Values: Numeric\nImputation Strategy: Since it is a continuous numerical variable, we use median imputation to prevent extreme values from affecting the distribution.","metadata":{"editable":false}},{"cell_type":"code","source":"# Define imputation strategies\ncategorical_features = [\"Brand\", \"Material\", \"Size\", \"Laptop Compartment\", \"Waterproof\", \"Style\", \"Color\"]\nnumerical_features = [\"Weight Capacity (kg)\"]\n\n# Fill categorical missing values with mode (most frequent value)\nfor col in categorical_features:\n    train[col].fillna(train[col].mode()[0], inplace=True)\n    test[col].fillna(test[col].mode()[0], inplace=True)\n\n# Fill numerical missing values with median\nfor col in numerical_features:\n    train[col].fillna(train[col].median(), inplace=True)\n    test[col].fillna(test[col].median(), inplace=True)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the dataset into train and test sets\nX = train.drop(columns='Price')  # Assuming 'Price' is the target\ny = train['Price']\n\n\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define categorical and numerical columns\ncategorical_cols = X.select_dtypes(include=['object']).columns\nnumerical_cols = X.select_dtypes(exclude=['object']).columns\nprint(categorical_cols)\nprint(numerical_cols)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode categorical variables using Label Encoding\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X_train[col] = le.fit_transform(X_train[col])\n    X_test[col] = le.transform(X_test[col])  # Important to use the same encoding for both train and test sets\n    label_encoders[col] = le\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import KFold\n# from sklearn.metrics import mean_squared_error\n# import numpy as np\n# import pandas as pd\n# from sklearn.linear_model import LinearRegression  # Import Linear Regression\n\n# # Ensure preprocessor is defined (Replace with actual preprocessing steps)\n# # Example: preprocessor = SomePreprocessingStep()\n# assert 'preprocessor' in globals(), \"Define 'preprocessor' before using it in the pipeline.\"\n\n# # Initialize KFold\n# n_folds = 5\n# kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# # DataFrame to store out-of-fold predictions\n# oof_df = pd.DataFrame(columns=['ID', 'Actual', 'OOF_Pred_LR', 'Fold'])\n\n# # Convert X_train and y_train to DataFrames if they aren't already\n# X_train = pd.DataFrame(X_train)\n# y_train = pd.Series(y_train)\n\n# # Initialize array to store out-of-fold predictions\n# oof_preds = np.zeros(len(X_train))\n\n# # K-Fold Cross-Validation Loop\n# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n#     print(f\"\\n🔄 Training Fold {fold}/{n_folds}...\")\n\n#     # Split data into training and validation sets\n#     X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n#     # Create pipeline with preprocessor and Linear Regression model\n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),  # Replace with actual preprocessor\n#         ('model', LinearRegression())  # Replace with Linear Regression\n#     ])\n\n#     # Train the model\n#     pipeline.fit(X_tr, y_tr)\n\n#     # Predict on validation set (OOF predictions)\n#     y_val_pred = pipeline.predict(X_val)\n#     oof_preds[val_idx] = y_val_pred  # Store OOF predictions\n\n#     # Compute fold RMSE\n#     fold_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n#     print(f\"✅ Fold {fold} RMSE: {fold_rmse:.4f}\")\n\n#     # Store fold results in DataFrame\n#     fold_df = pd.DataFrame({\n#         'ID': X_train.index[val_idx],  # Assuming index represents unique IDs\n#         'Actual': y_val.values,\n#         'OOF_Pred_LR': y_val_pred,\n#         'Fold': fold\n#     })\n\n#     oof_df = pd.concat([oof_df, fold_df], ignore_index=True)\n\n# # Compute overall OOF RMSE\n# oof_rmse = mean_squared_error(y_train, oof_preds, squared=False)\n# print(f\"\\n🏆 Overall OOF RMSE: {oof_rmse:.4f}\")\n\n# # Save OOF predictions to CSV\n# oof_df.to_csv('oof_predictions_lr.csv', index=False)\n# print(\"📂 OOF predictions saved to 'oof_predictions_lr.csv'.\")\n\n# # Display first few rows of OOF predictions\n# oof_df.reset_index(drop=True, inplace=True)\n# print(oof_df.head())\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Make predictions on the test set\n# predictions = pipeline.predict(test)\n\n# # Create the submission DataFrame\n# submission = pd.DataFrame({\n#     'id': test['id'],         # Ensure 'id' exists in the test set\n#     'Price': predictions      # Use predictions on the test set\n# })\n\n# # Save the DataFrame to a CSV file\n# submission.to_csv('submission200.csv', index=False)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.impute import SimpleImputer\n# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import KFold, GridSearchCV\n# from sklearn.metrics import mean_squared_error\n# from sklearn.linear_model import Ridge\n# import numpy as np\n# import pandas as pd\n\n# # Ensure preprocessor is defined (Replace with actual preprocessing steps)\n# assert 'preprocessor' in globals(), \"Define 'preprocessor' before using it in the pipeline.\"\n\n# # Define columns\n# numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n# categorical_features = X_train.select_dtypes(include=['object']).columns\n\n# # Create numeric transformer\n# numeric_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='mean'))  # Impute missing values with mean for numeric features\n# ])\n\n# # Create categorical transformer\n# categorical_transformer = Pipeline(steps=[\n#     ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent for categorical features\n#     ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding for categorical features\n# ])\n\n# # Combine both transformations using ColumnTransformer\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numeric_transformer, numeric_features),\n#         ('cat', categorical_transformer, categorical_features)\n#     ])\n\n# # Initialize KFold\n# n_folds = 5\n# kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# # DataFrame to store out-of-fold predictions\n# oof_df = pd.DataFrame(columns=['ID', 'Actual', 'OOF_Pred_Ridge', 'Fold'])\n\n# # Convert X_train and y_train to DataFrames if they aren't already\n# X_train = pd.DataFrame(X_train)\n# y_train = pd.Series(y_train)\n\n# # Initialize array to store out-of-fold predictions\n# oof_preds = np.zeros(len(X_train))\n\n# # Ridge hyperparameter grid for tuning alpha\n# param_grid = {\n#     'model__alpha': [0.01, 0.1, 1, 10, 100]  # Regularization strength (alpha)\n# }\n\n# # K-Fold Cross-Validation Loop\n# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n#     print(f\"\\n🔄 Training Fold {fold}/{n_folds}...\")\n\n#     # Split data into training and validation sets\n#     X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n#     # Create pipeline with imputation, preprocessor, and Ridge regression model\n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),  # Apply transformations to numeric and categorical features\n#         ('model', Ridge())  # Use Ridge regression instead of Linear Regression\n#     ])\n\n#     # Set up GridSearchCV for hyperparameter tuning\n#     grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n\n#     # Train the model using grid search\n#     grid_search.fit(X_tr, y_tr)\n\n#     # Best model after tuning\n#     best_model = grid_search.best_estimator_\n\n#     # Predict on validation set (OOF predictions) using the best model\n#     y_val_pred = best_model.predict(X_val)\n#     oof_preds[val_idx] = y_val_pred  # Store OOF predictions\n\n#     # Compute fold RMSE\n#     fold_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n#     print(f\"✅ Fold {fold} RMSE: {fold_rmse:.4f}\")\n\n#     # Store fold results in DataFrame\n#     fold_df = pd.DataFrame({\n#         'ID': X_train.index[val_idx],  # Assuming index represents unique IDs\n#         'Actual': y_val.values,\n#         'OOF_Pred_Ridge': y_val_pred,\n#         'Fold': fold\n#     })\n\n#     oof_df = pd.concat([oof_df, fold_df], ignore_index=True)\n\n# # Compute overall OOF RMSE\n# oof_rmse = mean_squared_error(y_train, oof_preds, squared=False)\n# print(f\"\\n🏆 Overall OOF RMSE: {oof_rmse:.4f}\")\n\n# # Save OOF predictions to CSV\n# oof_df.to_csv('oof_predictions_ridge.csv', index=False)\n# print(\"📂 OOF predictions saved to 'oof_predictions_ridge.csv'.\")\n\n# # Display first few rows of OOF predictions\n# oof_df.reset_index(drop=True, inplace=True)\n# print(oof_df.head())\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # After training and obtaining out-of-fold predictions (as you're already doing)\n# # Make predictions on the test set (Replace 'test' with your actual test DataFrame)\n# test = pd.DataFrame(test)  # Ensure test is a DataFrame\n# assert 'id' in test.columns, \"The test DataFrame must contain an 'id' column.\"\n\n# # Make predictions on the test set\n# test_predictions = best_model.predict(test)  # Make predictions using the best model after GridSearchCV\n\n# # Create the submission DataFrame\n# submission = pd.DataFrame({\n#     'id': test['id'],         # Ensure 'id' exists in the test set\n#     'Price': test_predictions  # Use predictions on the test set\n# })\n\n# # Save the DataFrame to a CSV file\n# submission.to_csv('submissiono0101.csv', index=False)\n\n# # Save the OOF predictions DataFrame to a CSV file\n# oof_df.to_csv('oof_predictions_ridge.csv', index=False)\n# print(\"📂 OOF predictions saved to 'oof_predictions_ridge.csv'.\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}