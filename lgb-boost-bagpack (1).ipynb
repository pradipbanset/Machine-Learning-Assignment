{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10757210,"sourceType":"datasetVersion","datasetId":6672301}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:50.347928Z","iopub.execute_input":"2025-02-22T19:19:50.348222Z","iopub.status.idle":"2025-02-22T19:19:50.355145Z","shell.execute_reply.started":"2025-02-22T19:19:50.348201Z","shell.execute_reply":"2025-02-22T19:19:50.354404Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bagpack/sample_submission.csv\n/kaggle/input/bagpack/train.csv\n/kaggle/input/bagpack/test.csv\n/kaggle/input/bagpack/training_extra.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Importing essential libraries\nimport numpy as np  # For numerical computations\nimport pandas as pd  # For data manipulation and analysis\nimport matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\nimport seaborn as sns  # For statistical data visualization\nimport warnings  # For controlling warning messages\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport optuna\n\n\n# Suppressing warnings to avoid clutter in output\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:52.422709Z","iopub.execute_input":"2025-02-22T19:19:52.423020Z","iopub.status.idle":"2025-02-22T19:19:52.427512Z","shell.execute_reply.started":"2025-02-22T19:19:52.422996Z","shell.execute_reply":"2025-02-22T19:19:52.426596Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/bagpack/train.csv')\ntrain_extra=pd.read_csv('/kaggle/input/bagpack/training_extra.csv')\ntest=pd.read_csv('/kaggle/input/bagpack/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:53.406291Z","iopub.execute_input":"2025-02-22T19:19:53.406569Z","iopub.status.idle":"2025-02-22T19:19:59.057127Z","shell.execute_reply.started":"2025-02-22T19:19:53.406547Z","shell.execute_reply":"2025-02-22T19:19:59.056399Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train.shape, test.shape, train_extra.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:59.058240Z","iopub.execute_input":"2025-02-22T19:19:59.058460Z","iopub.status.idle":"2025-02-22T19:19:59.063542Z","shell.execute_reply.started":"2025-02-22T19:19:59.058441Z","shell.execute_reply":"2025-02-22T19:19:59.062738Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"((300000, 11), (200000, 10), (3694318, 11))"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"train = pd.concat([train, train_extra], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:59.065048Z","iopub.execute_input":"2025-02-22T19:19:59.065306Z","iopub.status.idle":"2025-02-22T19:19:59.487907Z","shell.execute_reply.started":"2025-02-22T19:19:59.065285Z","shell.execute_reply":"2025-02-22T19:19:59.487078Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:59.488936Z","iopub.execute_input":"2025-02-22T19:19:59.489169Z","iopub.status.idle":"2025-02-22T19:19:59.493806Z","shell.execute_reply.started":"2025-02-22T19:19:59.489137Z","shell.execute_reply":"2025-02-22T19:19:59.492963Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(3994318, 11)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"train.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:19:59.494603Z","iopub.execute_input":"2025-02-22T19:19:59.494887Z","iopub.status.idle":"2025-02-22T19:19:59.616922Z","shell.execute_reply.started":"2025-02-22T19:19:59.494867Z","shell.execute_reply":"2025-02-22T19:19:59.616211Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"              id         Brand   Material    Size  Compartments  \\\n151924    151924  Under Armour      Nylon   Large           4.0   \n3972497  4172497          Nike      Nylon  Medium           4.0   \n1845394  2045394          Puma  Polyester   Large           4.0   \n1372122  1572122      Jansport     Canvas   Large           6.0   \n2313252  2513252           NaN  Polyester   Small           1.0   \n\n        Laptop Compartment Waterproof      Style  Color  Weight Capacity (kg)  \\\n151924                 Yes         No   Backpack   Gray             23.855925   \n3972497                 No        Yes       Tote   Gray             14.556469   \n1845394                 No        Yes   Backpack  Green              6.717409   \n1372122                 No         No       Tote   Gray              6.934273   \n2313252                Yes         No  Messenger   Blue             15.619607   \n\n             Price  \n151924   120.23094  \n3972497  136.80434  \n1845394   86.36257  \n1372122   49.42768  \n2313252  147.91700  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Brand</th>\n      <th>Material</th>\n      <th>Size</th>\n      <th>Compartments</th>\n      <th>Laptop Compartment</th>\n      <th>Waterproof</th>\n      <th>Style</th>\n      <th>Color</th>\n      <th>Weight Capacity (kg)</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>151924</th>\n      <td>151924</td>\n      <td>Under Armour</td>\n      <td>Nylon</td>\n      <td>Large</td>\n      <td>4.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Backpack</td>\n      <td>Gray</td>\n      <td>23.855925</td>\n      <td>120.23094</td>\n    </tr>\n    <tr>\n      <th>3972497</th>\n      <td>4172497</td>\n      <td>Nike</td>\n      <td>Nylon</td>\n      <td>Medium</td>\n      <td>4.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Tote</td>\n      <td>Gray</td>\n      <td>14.556469</td>\n      <td>136.80434</td>\n    </tr>\n    <tr>\n      <th>1845394</th>\n      <td>2045394</td>\n      <td>Puma</td>\n      <td>Polyester</td>\n      <td>Large</td>\n      <td>4.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Backpack</td>\n      <td>Green</td>\n      <td>6.717409</td>\n      <td>86.36257</td>\n    </tr>\n    <tr>\n      <th>1372122</th>\n      <td>1572122</td>\n      <td>Jansport</td>\n      <td>Canvas</td>\n      <td>Large</td>\n      <td>6.0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Tote</td>\n      <td>Gray</td>\n      <td>6.934273</td>\n      <td>49.42768</td>\n    </tr>\n    <tr>\n      <th>2313252</th>\n      <td>2513252</td>\n      <td>NaN</td>\n      <td>Polyester</td>\n      <td>Small</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Blue</td>\n      <td>15.619607</td>\n      <td>147.91700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:03.393067Z","iopub.execute_input":"2025-02-22T19:20:03.393341Z","iopub.status.idle":"2025-02-22T19:20:03.402100Z","shell.execute_reply.started":"2025-02-22T19:20:03.393321Z","shell.execute_reply":"2025-02-22T19:20:03.401415Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3994318 entries, 0 to 3994317\nData columns (total 11 columns):\n #   Column                Dtype  \n---  ------                -----  \n 0   id                    int64  \n 1   Brand                 object \n 2   Material              object \n 3   Size                  object \n 4   Compartments          float64\n 5   Laptop Compartment    object \n 6   Waterproof            object \n 7   Style                 object \n 8   Color                 object \n 9   Weight Capacity (kg)  float64\n 10  Price                 float64\ndtypes: float64(3), int64(1), object(7)\nmemory usage: 335.2+ MB\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Add a column with the count of NaN values per row\n#train['nan_count'] = train.isna().sum(axis=1)\n#test['nan_count'] = test.isna().sum(axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:04.241402Z","iopub.execute_input":"2025-02-22T19:20:04.241728Z","iopub.status.idle":"2025-02-22T19:20:04.244774Z","shell.execute_reply.started":"2025-02-22T19:20:04.241703Z","shell.execute_reply":"2025-02-22T19:20:04.244082Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:06.556696Z","iopub.execute_input":"2025-02-22T19:20:06.556997Z","iopub.status.idle":"2025-02-22T19:20:06.569851Z","shell.execute_reply.started":"2025-02-22T19:20:06.556975Z","shell.execute_reply":"2025-02-22T19:20:06.569168Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   id         Brand   Material    Size  Compartments Laptop Compartment  \\\n0   0      Jansport    Leather  Medium           7.0                Yes   \n1   1      Jansport     Canvas   Small          10.0                Yes   \n2   2  Under Armour    Leather   Small           2.0                Yes   \n3   3          Nike      Nylon   Small           8.0                Yes   \n4   4        Adidas     Canvas  Medium           1.0                Yes   \n5   5          Nike     Canvas  Medium          10.0                 No   \n6   6          Nike        NaN   Large           3.0                 No   \n7   7          Puma     Canvas   Small           1.0                Yes   \n8   8  Under Armour  Polyester  Medium           8.0                Yes   \n9   9  Under Armour      Nylon  Medium           2.0                Yes   \n\n  Waterproof      Style  Color  Weight Capacity (kg)      Price  \n0         No       Tote  Black             11.611723  112.15875  \n1        Yes  Messenger  Green             27.078537   68.88056  \n2         No  Messenger    Red             16.643760   39.17320  \n3         No  Messenger  Green             12.937220   80.60793  \n4        Yes  Messenger  Green             17.749338   86.02312  \n5        Yes        NaN  Black              7.241812   20.01553  \n6         No   Backpack  Green              6.828123   84.80500  \n7        Yes   Backpack   Blue             21.488864   27.15815  \n8         No       Tote   Gray             10.207780   25.98652  \n9        Yes  Messenger   Pink             15.895100   38.48741  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Brand</th>\n      <th>Material</th>\n      <th>Size</th>\n      <th>Compartments</th>\n      <th>Laptop Compartment</th>\n      <th>Waterproof</th>\n      <th>Style</th>\n      <th>Color</th>\n      <th>Weight Capacity (kg)</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Jansport</td>\n      <td>Leather</td>\n      <td>Medium</td>\n      <td>7.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Tote</td>\n      <td>Black</td>\n      <td>11.611723</td>\n      <td>112.15875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Jansport</td>\n      <td>Canvas</td>\n      <td>Small</td>\n      <td>10.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>27.078537</td>\n      <td>68.88056</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Under Armour</td>\n      <td>Leather</td>\n      <td>Small</td>\n      <td>2.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Red</td>\n      <td>16.643760</td>\n      <td>39.17320</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Nike</td>\n      <td>Nylon</td>\n      <td>Small</td>\n      <td>8.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>12.937220</td>\n      <td>80.60793</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Adidas</td>\n      <td>Canvas</td>\n      <td>Medium</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>17.749338</td>\n      <td>86.02312</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Nike</td>\n      <td>Canvas</td>\n      <td>Medium</td>\n      <td>10.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>Black</td>\n      <td>7.241812</td>\n      <td>20.01553</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Nike</td>\n      <td>NaN</td>\n      <td>Large</td>\n      <td>3.0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Backpack</td>\n      <td>Green</td>\n      <td>6.828123</td>\n      <td>84.80500</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Puma</td>\n      <td>Canvas</td>\n      <td>Small</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Backpack</td>\n      <td>Blue</td>\n      <td>21.488864</td>\n      <td>27.15815</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>Under Armour</td>\n      <td>Polyester</td>\n      <td>Medium</td>\n      <td>8.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Tote</td>\n      <td>Gray</td>\n      <td>10.207780</td>\n      <td>25.98652</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>Under Armour</td>\n      <td>Nylon</td>\n      <td>Medium</td>\n      <td>2.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Pink</td>\n      <td>15.895100</td>\n      <td>38.48741</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"cat_cols=train.select_dtypes(include='object').columns.tolist()\ncat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:07.625018Z","iopub.execute_input":"2025-02-22T19:20:07.625283Z","iopub.status.idle":"2025-02-22T19:20:08.773015Z","shell.execute_reply.started":"2025-02-22T19:20:07.625263Z","shell.execute_reply":"2025-02-22T19:20:08.772259Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['Brand',\n 'Material',\n 'Size',\n 'Laptop Compartment',\n 'Waterproof',\n 'Style',\n 'Color']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(train.drop(columns='Price'),train['Price'],test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:08.773811Z","iopub.execute_input":"2025-02-22T19:20:08.774014Z","iopub.status.idle":"2025-02-22T19:20:11.343386Z","shell.execute_reply.started":"2025-02-22T19:20:08.773997Z","shell.execute_reply":"2025-02-22T19:20:11.342704Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"Creating dataprocessing pipeline\nThis code builds a data preprocessing pipeline that prepares numerical and categorical features for a machine learning model. It does the following:\n\nStandardizes numerical data (specifically Weight Capacity (kg)) using StandardScaler(), ensuring consistent scaling.\nEncodes categorical features (cat_cols) using OneHotEncoder(), converting them into a machine-readable format.\nLeaves any remaining columns unchanged to retain necessary data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:20:11.344524Z","iopub.execute_input":"2025-02-22T19:20:11.344759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport joblib\n\n# Define the pipeline components\nweight_capacity_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Define the column transformer for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('weight_capacity_pipe', weight_capacity_pipe, ['Weight Capacity (kg)']),  # Scaling numeric column\n        ('cat_pipeline', Pipeline(steps=[  # One-Hot Encoding categorical features\n            ('encoder', OneHotEncoder())\n        ]), cat_cols)\n    ],\n    remainder='passthrough'\n)\n\n# Save the preprocessor pipeline to a .pkl file\njoblib.dump(preprocessor, 'preprocessor_pipeline.pkl')\nprint(\"üìÇ Preprocessor pipeline saved to 'preprocessor_pipeline.pkl'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nimport joblib  # For saving the model\n\n# Define the best hyperparameters (Updated)\nbest_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.030198582603149962,\n    'max_depth': 6,\n    'num_leaves': 13,\n    'min_child_samples': 4,\n    'subsample': 0.8666752007530039,\n    'colsample_bytree': 0.8227478250119855,\n    'reg_alpha': 0.004905763778413513,\n    'reg_lambda': 0.0011562120556175835\n}\n\n# Ensure preprocessor is defined (Replace with actual preprocessing steps)\nassert 'preprocessor' in globals(), \"Define 'preprocessor' before using it in the pipeline.\"\n\n# Initialize KFold\nn_folds = 3\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# DataFrame to store out-of-fold predictions\noof_df = pd.DataFrame(columns=['ID', 'Actual', 'OOF_Pred_LGB', 'Fold'])\n\n# Convert X_train and y_train to DataFrames if they aren't already\nX_train = pd.DataFrame(X_train)\ny_train = pd.Series(y_train)\n\n# Initialize array to store out-of-fold predictions\noof_preds = np.zeros(len(X_train))\n\n# K-Fold Cross-Validation Loop\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n    print(f\"\\nüîÑ Training Fold {fold}/{n_folds}...\")\n\n    # Split data into training and validation sets\n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    # Create pipeline with preprocessor and LGBM model\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),  # Replace with actual preprocessor\n        ('model', LGBMRegressor(**best_params))\n    ])\n\n    # Train the model\n    pipeline.fit(X_tr, y_tr)\n\n    # Predict on validation set (OOF predictions)\n    y_val_pred = pipeline.predict(X_val)\n    oof_preds[val_idx] = y_val_pred  # Store OOF predictions\n\n    # Compute fold RMSE\n    fold_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n    print(f\"‚úÖ Fold {fold} RMSE: {fold_rmse:.4f}\")\n\n    # Store fold results in DataFrame\n    fold_df = pd.DataFrame({\n        'ID': X_train.index[val_idx],  # Assuming index represents unique IDs\n        'Actual': y_val.values,\n        'OOF_Pred_LGB': y_val_pred,\n        'Fold': fold\n    })\n\n    oof_df = pd.concat([oof_df, fold_df], ignore_index=True)\n\n# Compute overall OOF RMSE\noof_rmse = mean_squared_error(y_train, oof_preds, squared=False)\nprint(f\"\\nüèÜ Overall OOF RMSE: {oof_rmse:.4f}\")\n\n# Save OOF predictions to CSV\noof_df.to_csv('oof_predictions_lgbm.csv', index=False)\nprint(\"üìÇ OOF predictions saved to 'oof_predictions_lgbm.csv'.\")\n\n# Save the pipeline (model and preprocessor)\nmodel_filename = 'trained_model_lgbm.pkl'\njoblib.dump(pipeline, model_filename)\nprint(f\"üìÇ Model saved as {model_filename}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the pipeline (model and preprocessor)\nLGBoost = 'trained_model_lgbm.pkl'\njoblib.dump(pipeline, LGBoost)\nprint(f\"üìÇ Model saved as {LGBoost}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test set\npredictions = pipeline.predict(test)\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],         # Ensure 'id' exists in the test set\n    'Price': predictions      # Use predictions on the test set\n})\n\n# Save the DataFrame to a CSV file\nsubmission.to_csv('submission200.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming you still have X_train and y_train available\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Save X_val as CSV if needed\nX_val.to_csv('X_val.csv', index=False)\n\nX_train.to_csv('X_train.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# from lightgbm import LGBMRegressor\n# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import cross_val_score, KFold\n# from sklearn.metrics import mean_squared_error, make_scorer\n# import numpy as np\n\n# def objective(trial):\n#     params = {\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n#         'max_depth': trial.suggest_int('max_depth', 3, 7),\n#         'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 1, 10),\n#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1.0),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1.0)\n#     }\n    \n#     preprocessor = ColumnTransformer(\n#         transformers=[\n#             ('weight_capacity_pipe', Pipeline(steps=[('scaler', StandardScaler())]), ['Weight Capacity (kg)']),\n#             ('cat_pipeline', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n#         ],\n#         remainder='passthrough'\n#     )\n    \n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('model', LGBMRegressor(**params))\n#     ])\n    \n#     rmse_scorer = make_scorer(mean_squared_error, squared=False)\n#     kf = KFold(n_splits=3, shuffle=True, random_state=42)  # Reduced to 3 folds for faster execution\n#     scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=rmse_scorer)\n    \n#     return np.mean(scores)\n\n# study = optuna.create_study(direction='minimize')  # Minimize RMSE\n# study.optimize(objective, n_trials=3, n_jobs=-1)  # Perform 3 trials\n\n# best_params = study.best_params\n# print(f\"Best Parameters: {best_params}\")\n\n# final_model = LGBMRegressor(**best_params)\n# pipeline = Pipeline(steps=[\n#     ('preprocessor', preprocessor),\n#     ('model', final_model)\n# ])\n\n# pipeline.fit(X_train, y_train)\n\n# y_pred = pipeline.predict(X_test)\n# test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n# print(f'Test Set RMSE: {test_rmse:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# import numpy as np\n# from xgboost import XGBRegressor\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import train_test_split\n# from category_encoders import TargetEncoder\n# from sklearn.compose import ColumnTransformer\n# import pandas as pd\n\n# # Load dataset\n# X = train.drop(columns='Price')\n# y = train['Price']\n\n# # Identify categorical columns\n# cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# # Apply Target Encoding instead of OneHotEncoding\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('cat', TargetEncoder(), cat_cols)\n#     ], remainder='passthrough'\n# )\n\n# X = preprocessor.fit_transform(X, y)\n\n# # Split data for validation\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Objective function for Optuna\n# def objective(trial):\n#     param = {\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 3000, step=100),  # Reduced max estimators\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),  # Adjusted range\n#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n#         'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n#         'gamma': trial.suggest_uniform('gamma', 0, 1),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1e-1),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1e-1),\n#         'objective': 'reg:squarederror',\n#         'eval_metric': 'rmse',\n#         'n_jobs': -1  # Enable parallel computation\n#     }\n\n#     # Train the model with early stopping\n#     model = XGBRegressor(**param)\n#     model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=False)\n    \n#     # Predictions and evaluation\n#     preds = model.predict(X_valid)\n#     rmse = mean_squared_error(y_valid, preds, squared=False)\n#     return rmse\n\n# # Create Optuna study\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=5)  # Reduce trials for faster training\n\n# # Get the best hyperparameters\n# best_params = study.best_params\n# print(f\"Best hyperparameters: {best_params}\")\n\n# # Train the final model with the best hyperparameters\n# best_model = XGBRegressor(**best_params, n_jobs=-1)\n# best_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}