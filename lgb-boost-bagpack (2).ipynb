{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10757210,"sourceType":"datasetVersion","datasetId":6672301}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:58.083831Z","iopub.execute_input":"2025-02-22T19:04:58.084117Z","iopub.status.idle":"2025-02-22T19:04:58.394761Z","shell.execute_reply.started":"2025-02-22T19:04:58.084068Z","shell.execute_reply":"2025-02-22T19:04:58.393883Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bagpack/sample_submission.csv\n/kaggle/input/bagpack/train.csv\n/kaggle/input/bagpack/test.csv\n/kaggle/input/bagpack/training_extra.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Importing essential libraries\nimport numpy as np  # For numerical computations\nimport pandas as pd  # For data manipulation and analysis\nimport matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\nimport seaborn as sns  # For statistical data visualization\nimport warnings  # For controlling warning messages\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nimport optuna\n\n\n# Suppressing warnings to avoid clutter in output\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:04:58.395641Z","iopub.execute_input":"2025-02-22T19:04:58.396083Z","iopub.status.idle":"2025-02-22T19:05:00.052987Z","shell.execute_reply.started":"2025-02-22T19:04:58.396052Z","shell.execute_reply":"2025-02-22T19:05:00.052145Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/bagpack/train.csv')\ntrain_extra=pd.read_csv('/kaggle/input/bagpack/training_extra.csv')\ntest=pd.read_csv('/kaggle/input/bagpack/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:02.731181Z","iopub.execute_input":"2025-02-22T19:05:02.731546Z","iopub.status.idle":"2025-02-22T19:05:11.176710Z","shell.execute_reply.started":"2025-02-22T19:05:02.731520Z","shell.execute_reply":"2025-02-22T19:05:11.175849Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train.shape, test.shape, train_extra.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.177659Z","iopub.execute_input":"2025-02-22T19:05:11.177889Z","iopub.status.idle":"2025-02-22T19:05:11.183696Z","shell.execute_reply.started":"2025-02-22T19:05:11.177871Z","shell.execute_reply":"2025-02-22T19:05:11.182911Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((300000, 11), (200000, 10), (3694318, 11))"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train = pd.concat([train, train_extra], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.185086Z","iopub.execute_input":"2025-02-22T19:05:11.185299Z","iopub.status.idle":"2025-02-22T19:05:11.614081Z","shell.execute_reply.started":"2025-02-22T19:05:11.185269Z","shell.execute_reply":"2025-02-22T19:05:11.613376Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.615263Z","iopub.execute_input":"2025-02-22T19:05:11.615490Z","iopub.status.idle":"2025-02-22T19:05:11.620072Z","shell.execute_reply.started":"2025-02-22T19:05:11.615473Z","shell.execute_reply":"2025-02-22T19:05:11.619324Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(3994318, 11)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.620882Z","iopub.execute_input":"2025-02-22T19:05:11.621166Z","iopub.status.idle":"2025-02-22T19:05:11.802453Z","shell.execute_reply.started":"2025-02-22T19:05:11.621139Z","shell.execute_reply":"2025-02-22T19:05:11.801757Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"              id     Brand Material    Size  Compartments Laptop Compartment  \\\n2155743  2355743  Jansport  Leather  Medium           6.0                 No   \n332675    532675      Nike  Leather   Small           9.0                 No   \n2159732  2359732      Puma   Canvas   Large           2.0                 No   \n3755091  3955091    Adidas      NaN   Large           8.0                Yes   \n1557259  1757259      Nike   Canvas  Medium           4.0                 No   \n\n        Waterproof      Style  Color  Weight Capacity (kg)      Price  \n2155743         No   Backpack  Black             22.144059   30.63479  \n332675         Yes   Backpack   Blue             26.793464   38.08527  \n2159732         No  Messenger   Pink             17.955198   47.82741  \n3755091         No   Backpack   Pink             22.381331   29.34273  \n1557259        Yes  Messenger  Black             24.276908  109.60189  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Brand</th>\n      <th>Material</th>\n      <th>Size</th>\n      <th>Compartments</th>\n      <th>Laptop Compartment</th>\n      <th>Waterproof</th>\n      <th>Style</th>\n      <th>Color</th>\n      <th>Weight Capacity (kg)</th>\n      <th>Price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2155743</th>\n      <td>2355743</td>\n      <td>Jansport</td>\n      <td>Leather</td>\n      <td>Medium</td>\n      <td>6.0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Backpack</td>\n      <td>Black</td>\n      <td>22.144059</td>\n      <td>30.63479</td>\n    </tr>\n    <tr>\n      <th>332675</th>\n      <td>532675</td>\n      <td>Nike</td>\n      <td>Leather</td>\n      <td>Small</td>\n      <td>9.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Backpack</td>\n      <td>Blue</td>\n      <td>26.793464</td>\n      <td>38.08527</td>\n    </tr>\n    <tr>\n      <th>2159732</th>\n      <td>2359732</td>\n      <td>Puma</td>\n      <td>Canvas</td>\n      <td>Large</td>\n      <td>2.0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Pink</td>\n      <td>17.955198</td>\n      <td>47.82741</td>\n    </tr>\n    <tr>\n      <th>3755091</th>\n      <td>3955091</td>\n      <td>Adidas</td>\n      <td>NaN</td>\n      <td>Large</td>\n      <td>8.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Backpack</td>\n      <td>Pink</td>\n      <td>22.381331</td>\n      <td>29.34273</td>\n    </tr>\n    <tr>\n      <th>1557259</th>\n      <td>1757259</td>\n      <td>Nike</td>\n      <td>Canvas</td>\n      <td>Medium</td>\n      <td>4.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Black</td>\n      <td>24.276908</td>\n      <td>109.60189</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.803123Z","iopub.execute_input":"2025-02-22T19:05:11.803337Z","iopub.status.idle":"2025-02-22T19:05:11.821905Z","shell.execute_reply.started":"2025-02-22T19:05:11.803320Z","shell.execute_reply":"2025-02-22T19:05:11.821077Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3994318 entries, 0 to 3994317\nData columns (total 11 columns):\n #   Column                Dtype  \n---  ------                -----  \n 0   id                    int64  \n 1   Brand                 object \n 2   Material              object \n 3   Size                  object \n 4   Compartments          float64\n 5   Laptop Compartment    object \n 6   Waterproof            object \n 7   Style                 object \n 8   Color                 object \n 9   Weight Capacity (kg)  float64\n 10  Price                 float64\ndtypes: float64(3), int64(1), object(7)\nmemory usage: 335.2+ MB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Add a column with the count of NaN values per row\ntrain['nan_count'] = train.isna().sum(axis=1)\ntest['nan_count'] = test.isna().sum(axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:11.822672Z","iopub.execute_input":"2025-02-22T19:05:11.822874Z","iopub.status.idle":"2025-02-22T19:05:13.517083Z","shell.execute_reply.started":"2025-02-22T19:05:11.822856Z","shell.execute_reply":"2025-02-22T19:05:13.516553Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:13.519219Z","iopub.execute_input":"2025-02-22T19:05:13.519449Z","iopub.status.idle":"2025-02-22T19:05:13.596144Z","shell.execute_reply.started":"2025-02-22T19:05:13.519430Z","shell.execute_reply":"2025-02-22T19:05:13.595280Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   id         Brand   Material    Size  Compartments Laptop Compartment  \\\n0   0      Jansport    Leather  Medium           7.0                Yes   \n1   1      Jansport     Canvas   Small          10.0                Yes   \n2   2  Under Armour    Leather   Small           2.0                Yes   \n3   3          Nike      Nylon   Small           8.0                Yes   \n4   4        Adidas     Canvas  Medium           1.0                Yes   \n5   5          Nike     Canvas  Medium          10.0                 No   \n6   6          Nike        NaN   Large           3.0                 No   \n7   7          Puma     Canvas   Small           1.0                Yes   \n8   8  Under Armour  Polyester  Medium           8.0                Yes   \n9   9  Under Armour      Nylon  Medium           2.0                Yes   \n\n  Waterproof      Style  Color  Weight Capacity (kg)      Price  nan_count  \n0         No       Tote  Black             11.611723  112.15875          0  \n1        Yes  Messenger  Green             27.078537   68.88056          0  \n2         No  Messenger    Red             16.643760   39.17320          0  \n3         No  Messenger  Green             12.937220   80.60793          0  \n4        Yes  Messenger  Green             17.749338   86.02312          0  \n5        Yes        NaN  Black              7.241812   20.01553          1  \n6         No   Backpack  Green              6.828123   84.80500          1  \n7        Yes   Backpack   Blue             21.488864   27.15815          0  \n8         No       Tote   Gray             10.207780   25.98652          0  \n9        Yes  Messenger   Pink             15.895100   38.48741          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Brand</th>\n      <th>Material</th>\n      <th>Size</th>\n      <th>Compartments</th>\n      <th>Laptop Compartment</th>\n      <th>Waterproof</th>\n      <th>Style</th>\n      <th>Color</th>\n      <th>Weight Capacity (kg)</th>\n      <th>Price</th>\n      <th>nan_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Jansport</td>\n      <td>Leather</td>\n      <td>Medium</td>\n      <td>7.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Tote</td>\n      <td>Black</td>\n      <td>11.611723</td>\n      <td>112.15875</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Jansport</td>\n      <td>Canvas</td>\n      <td>Small</td>\n      <td>10.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>27.078537</td>\n      <td>68.88056</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Under Armour</td>\n      <td>Leather</td>\n      <td>Small</td>\n      <td>2.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Red</td>\n      <td>16.643760</td>\n      <td>39.17320</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Nike</td>\n      <td>Nylon</td>\n      <td>Small</td>\n      <td>8.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>12.937220</td>\n      <td>80.60793</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Adidas</td>\n      <td>Canvas</td>\n      <td>Medium</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Green</td>\n      <td>17.749338</td>\n      <td>86.02312</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Nike</td>\n      <td>Canvas</td>\n      <td>Medium</td>\n      <td>10.0</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>NaN</td>\n      <td>Black</td>\n      <td>7.241812</td>\n      <td>20.01553</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Nike</td>\n      <td>NaN</td>\n      <td>Large</td>\n      <td>3.0</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Backpack</td>\n      <td>Green</td>\n      <td>6.828123</td>\n      <td>84.80500</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Puma</td>\n      <td>Canvas</td>\n      <td>Small</td>\n      <td>1.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Backpack</td>\n      <td>Blue</td>\n      <td>21.488864</td>\n      <td>27.15815</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>Under Armour</td>\n      <td>Polyester</td>\n      <td>Medium</td>\n      <td>8.0</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Tote</td>\n      <td>Gray</td>\n      <td>10.207780</td>\n      <td>25.98652</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>Under Armour</td>\n      <td>Nylon</td>\n      <td>Medium</td>\n      <td>2.0</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Messenger</td>\n      <td>Pink</td>\n      <td>15.895100</td>\n      <td>38.48741</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"cat_cols=train.select_dtypes(include='object').columns.tolist()\ncat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:13.597172Z","iopub.execute_input":"2025-02-22T19:05:13.597486Z","iopub.status.idle":"2025-02-22T19:05:14.671831Z","shell.execute_reply.started":"2025-02-22T19:05:13.597457Z","shell.execute_reply":"2025-02-22T19:05:14.670946Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Brand',\n 'Material',\n 'Size',\n 'Laptop Compartment',\n 'Waterproof',\n 'Style',\n 'Color']"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(train.drop(columns='Price'),train['Price'],test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:14.672744Z","iopub.execute_input":"2025-02-22T19:05:14.673075Z","iopub.status.idle":"2025-02-22T19:05:17.224778Z","shell.execute_reply.started":"2025-02-22T19:05:14.673045Z","shell.execute_reply":"2025-02-22T19:05:17.224119Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Creating dataprocessing pipeline\nThis code builds a data preprocessing pipeline that prepares numerical and categorical features for a machine learning model. It does the following:\n\nStandardizes numerical data (specifically Weight Capacity (kg)) using StandardScaler(), ensuring consistent scaling.\nEncodes categorical features (cat_cols) using OneHotEncoder(), converting them into a machine-readable format.\nLeaves any remaining columns unchanged to retain necessary data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:17.225590Z","iopub.execute_input":"2025-02-22T19:05:17.225887Z","iopub.status.idle":"2025-02-22T19:05:20.704727Z","shell.execute_reply.started":"2025-02-22T19:05:17.225860Z","shell.execute_reply":"2025-02-22T19:05:20.704078Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport joblib\n\n# Define the pipeline components\nweight_capacity_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Define the column transformer for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('weight_capacity_pipe', weight_capacity_pipe, ['Weight Capacity (kg)']),  # Scaling numeric column\n        ('cat_pipeline', Pipeline(steps=[  # One-Hot Encoding categorical features\n            ('encoder', OneHotEncoder())\n        ]), cat_cols)\n    ],\n    remainder='passthrough'\n)\n\n# Save the preprocessor pipeline to a .pkl file\njoblib.dump(preprocessor, 'preprocessor_pipeline.pkl')\nprint(\"üìÇ Preprocessor pipeline saved to 'preprocessor_pipeline.pkl'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nimport joblib  # For saving the model\n\n# Define the best hyperparameters (Updated)\nbest_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.030198582603149962,\n    'max_depth': 6,\n    'num_leaves': 13,\n    'min_child_samples': 4,\n    'subsample': 0.8666752007530039,\n    'colsample_bytree': 0.8227478250119855,\n    'reg_alpha': 0.004905763778413513,\n    'reg_lambda': 0.0011562120556175835\n}\n\n# Ensure preprocessor is defined (Replace with actual preprocessing steps)\n#assert 'preprocessor' in globals(), \"Define 'preprocessor' before using it in the pipeline.\"\n# Define the pipeline components\nweight_capacity_pipe = Pipeline(steps=[('scaler', StandardScaler())])\n\n# Define the column transformer for preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('weight_capacity_pipe', weight_capacity_pipe, ['Weight Capacity (kg)']),  # Scaling numeric column\n        ('cat_pipeline', Pipeline(steps=[  # One-Hot Encoding categorical features\n            ('encoder', OneHotEncoder())\n        ]), cat_cols)\n    ],\n    remainder='passthrough'\n)\n\n\n# Initialize KFold\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# DataFrame to store out-of-fold predictions\noof_df = pd.DataFrame(columns=['ID', 'Actual', 'OOF_Pred_LGB', 'Fold'])\n\n# Convert X_train and y_train to DataFrames if they aren't already\nX_train = pd.DataFrame(X_train)\ny_train = pd.Series(y_train)\n\n# Initialize array to store out-of-fold predictions\noof_preds = np.zeros(len(X_train))\n\n# K-Fold Cross-Validation Loop\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n    print(f\"\\nüîÑ Training Fold {fold}/{n_folds}...\")\n\n    # Split data into training and validation sets\n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    # Create pipeline with preprocessor and LGBM model\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),  # Replace with actual preprocessor\n        ('model', LGBMRegressor(**best_params))\n    ])\n\n    # Train the model\n    pipeline.fit(X_tr, y_tr)\n\n    # Predict on validation set (OOF predictions)\n    y_val_pred = pipeline.predict(X_val)\n    oof_preds[val_idx] = y_val_pred  # Store OOF predictions\n\n    # Compute fold RMSE\n    fold_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n    print(f\"‚úÖ Fold {fold} RMSE: {fold_rmse:.4f}\")\n\n    # Store fold results in DataFrame\n    fold_df = pd.DataFrame({\n        'ID': X_train.index[val_idx],  # Assuming index represents unique IDs\n        'Actual': y_val.values,\n        'OOF_Pred_LGB': y_val_pred,\n        'Fold': fold\n    })\n\n    oof_df = pd.concat([oof_df, fold_df], ignore_index=True)\n\n# Compute overall OOF RMSE\noof_rmse = mean_squared_error(y_train, oof_preds, squared=False)\nprint(f\"\\nüèÜ Overall OOF RMSE: {oof_rmse:.4f}\")\n\n# Save OOF predictions to CSV\noof_df.to_csv('oof_predictions_lgbm.csv', index=False)\nprint(\"üìÇ OOF predictions saved to 'oof_predictions_lgbm.csv'.\")\n\n# Save the pipeline (model and preprocessor)\nmodel_filename = 'trained_model_lgbm.pkl'\njoblib.dump(pipeline, model_filename)\nprint(f\"üìÇ Model saved as {model_filename}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T19:05:20.706108Z","iopub.execute_input":"2025-02-22T19:05:20.706731Z","iopub.status.idle":"2025-02-22T19:11:26.741423Z","shell.execute_reply.started":"2025-02-22T19:05:20.706709Z","shell.execute_reply":"2025-02-22T19:11:26.740530Z"}},"outputs":[{"name":"stdout","text":"\nüîÑ Training Fold 1/5...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047658 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 2556363, number of used features: 36\n[LightGBM] [Info] Start training from score 81.347753\n‚úÖ Fold 1 RMSE: 38.8690\n\nüîÑ Training Fold 2/5...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.076615 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 2556363, number of used features: 36\n[LightGBM] [Info] Start training from score 81.346180\n‚úÖ Fold 2 RMSE: 38.9015\n\nüîÑ Training Fold 3/5...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041809 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 590\n[LightGBM] [Info] Number of data points in the train set: 2556363, number of used features: 36\n[LightGBM] [Info] Start training from score 81.347156\n‚úÖ Fold 3 RMSE: 38.8811\n\nüîÑ Training Fold 4/5...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041429 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 2556363, number of used features: 36\n[LightGBM] [Info] Start training from score 81.346978\n‚úÖ Fold 4 RMSE: 38.8619\n\nüîÑ Training Fold 5/5...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046149 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 591\n[LightGBM] [Info] Number of data points in the train set: 2556364, number of used features: 36\n[LightGBM] [Info] Start training from score 81.380962\n‚úÖ Fold 5 RMSE: 38.8340\n\nüèÜ Overall OOF RMSE: 38.8695\nüìÇ OOF predictions saved to 'oof_predictions_lgbm.csv'.\nüìÇ Model saved as trained_model_lgbm.pkl.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nimport joblib  # For saving the model\n\n# Define the best hyperparameters (Updated)\nbest_params = {\n    'n_estimators': 1000,\n    'learning_rate': 0.030198582603149962,\n    'max_depth': 6,\n    'num_leaves': 13,\n    'min_child_samples': 4,\n    'subsample': 0.8666752007530039,\n    'colsample_bytree': 0.8227478250119855,\n    'reg_alpha': 0.004905763778413513,\n    'reg_lambda': 0.0011562120556175835\n}\n\n# Ensure preprocessor is defined (Replace with actual preprocessing steps)\nassert 'preprocessor' in globals(), \"Define 'preprocessor' before using it in the pipeline.\"\n\n# Initialize KFold\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# DataFrame to store out-of-fold predictions\noof_df = pd.DataFrame(columns=['ID', 'Actual', 'OOF_Pred_LGB', 'Fold'])\n\n# Convert X_train and y_train to DataFrames if they aren't already\nX_train = pd.DataFrame(X_train)\ny_train = pd.Series(y_train)\n\n# Initialize array to store out-of-fold predictions\noof_preds = np.zeros(len(X_train))\n\n# K-Fold Cross-Validation Loop\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), start=1):\n    print(f\"\\nüîÑ Training Fold {fold}/{n_folds}...\")\n\n    # Split data into training and validation sets\n    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n    # Create pipeline with preprocessor and LGBM model\n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),  # Replace with actual preprocessor\n        ('model', LGBMRegressor(**best_params))\n    ])\n\n    # Train the model\n    pipeline.fit(X_tr, y_tr)\n\n    # Predict on validation set (OOF predictions)\n    y_val_pred = pipeline.predict(X_val)\n    oof_preds[val_idx] = y_val_pred  # Store OOF predictions\n\n    # Compute fold RMSE\n    fold_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n    print(f\"‚úÖ Fold {fold} RMSE: {fold_rmse:.4f}\")\n\n    # Store fold results in DataFrame\n    fold_df = pd.DataFrame({\n        'ID': X_train.index[val_idx],  # Assuming index represents unique IDs\n        'Actual': y_val.values,\n        'OOF_Pred_LGB': y_val_pred,\n        'Fold': fold\n    })\n\n    oof_df = pd.concat([oof_df, fold_df], ignore_index=True)\n\n# Compute overall OOF RMSE\noof_rmse = mean_squared_error(y_train, oof_preds, squared=False)\nprint(f\"\\nüèÜ Overall OOF RMSE: {oof_rmse:.4f}\")\n\n# Save OOF predictions to CSV\noof_df.to_csv('oof_predictions_lgbm.csv', index=False)\nprint(\"üìÇ OOF predictions saved to 'oof_predictions_lgbm.csv'.\")\n\n# Save the pipeline (model and preprocessor)\nmodel_filename = 'trained_model_lgbm.pkl'\njoblib.dump(pipeline, model_filename)\nprint(f\"üìÇ Model saved as {model_filename}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the pipeline (model and preprocessor)\nLGBoost = 'trained_model_lgbm.pkl'\njoblib.dump(pipeline, LGBoost)\nprint(f\"üìÇ Model saved as {LGBoost}.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test set\npredictions = pipeline.predict(test)\n\n# Create the submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],         # Ensure 'id' exists in the test set\n    'Price': predictions      # Use predictions on the test set\n})\n\n# Save the DataFrame to a CSV file\nsubmission.to_csv('submission200.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming you still have X_train and y_train available\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Save X_val as CSV if needed\nX_val.to_csv('X_val.csv', index=False)\n\nX_train.to_csv('X_train.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# from lightgbm import LGBMRegressor\n# from sklearn.pipeline import Pipeline\n# from sklearn.model_selection import cross_val_score, KFold\n# from sklearn.metrics import mean_squared_error, make_scorer\n# import numpy as np\n\n# def objective(trial):\n#     params = {\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n#         'max_depth': trial.suggest_int('max_depth', 3, 7),\n#         'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 1, 10),\n#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1.0),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1.0)\n#     }\n    \n#     preprocessor = ColumnTransformer(\n#         transformers=[\n#             ('weight_capacity_pipe', Pipeline(steps=[('scaler', StandardScaler())]), ['Weight Capacity (kg)']),\n#             ('cat_pipeline', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n#         ],\n#         remainder='passthrough'\n#     )\n    \n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('model', LGBMRegressor(**params))\n#     ])\n    \n#     rmse_scorer = make_scorer(mean_squared_error, squared=False)\n#     kf = KFold(n_splits=3, shuffle=True, random_state=42)  # Reduced to 3 folds for faster execution\n#     scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring=rmse_scorer)\n    \n#     return np.mean(scores)\n\n# study = optuna.create_study(direction='minimize')  # Minimize RMSE\n# study.optimize(objective, n_trials=3, n_jobs=-1)  # Perform 3 trials\n\n# best_params = study.best_params\n# print(f\"Best Parameters: {best_params}\")\n\n# final_model = LGBMRegressor(**best_params)\n# pipeline = Pipeline(steps=[\n#     ('preprocessor', preprocessor),\n#     ('model', final_model)\n# ])\n\n# pipeline.fit(X_train, y_train)\n\n# y_pred = pipeline.predict(X_test)\n# test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n# print(f'Test Set RMSE: {test_rmse:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import optuna\n# import numpy as np\n# from xgboost import XGBRegressor\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import train_test_split\n# from category_encoders import TargetEncoder\n# from sklearn.compose import ColumnTransformer\n# import pandas as pd\n\n# # Load dataset\n# X = train.drop(columns='Price')\n# y = train['Price']\n\n# # Identify categorical columns\n# cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# # Apply Target Encoding instead of OneHotEncoding\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('cat', TargetEncoder(), cat_cols)\n#     ], remainder='passthrough'\n# )\n\n# X = preprocessor.fit_transform(X, y)\n\n# # Split data for validation\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Objective function for Optuna\n# def objective(trial):\n#     param = {\n#         'n_estimators': trial.suggest_int('n_estimators', 100, 3000, step=100),  # Reduced max estimators\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),  # Adjusted range\n#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n#         'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n#         'gamma': trial.suggest_uniform('gamma', 0, 1),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1e-1),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1e-1),\n#         'objective': 'reg:squarederror',\n#         'eval_metric': 'rmse',\n#         'n_jobs': -1  # Enable parallel computation\n#     }\n\n#     # Train the model with early stopping\n#     model = XGBRegressor(**param)\n#     model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=50, verbose=False)\n    \n#     # Predictions and evaluation\n#     preds = model.predict(X_valid)\n#     rmse = mean_squared_error(y_valid, preds, squared=False)\n#     return rmse\n\n# # Create Optuna study\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=5)  # Reduce trials for faster training\n\n# # Get the best hyperparameters\n# best_params = study.best_params\n# print(f\"Best hyperparameters: {best_params}\")\n\n# # Train the final model with the best hyperparameters\n# best_model = XGBRegressor(**best_params, n_jobs=-1)\n# best_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}